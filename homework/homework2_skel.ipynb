{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzPKYpndJ6Rv"
   },
   "source": [
    "NAME: __FULLNAME__\n",
    "\n",
    "# Homework 2\n",
    "## Machine Learning Practice, 2022\n",
    "\n",
    "### Objectives\n",
    "* Object orientation in Python\n",
    "* Constructing Data Pre-processing Pipelines\n",
    "  + Imputing\n",
    "  + Filtering\n",
    "  + Simple Numerical Methods\n",
    "  \n",
    " \n",
    "### Notes\n",
    "* Do not save work within the MLP_2022 folder\n",
    "  + create a folder in your home directory for assignments, and copy the skeleton there  \n",
    "\n",
    "### Hand-In Procedure\n",
    "* Execute all cells so they are showing correct results\n",
    "* Notebook (from Jupyter or Colab):\n",
    "  + Submit this file (.ipynb) to the Gradscope Notebook HW2 dropbox\n",
    "* Note: there is no need to submit a PDF file or to submit directly to Canvas\n",
    "  \n",
    "### General References\n",
    "(there are hints here)\n",
    "* [Sci-kit Learn Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "* [Sci-kit Learn Impute](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute)\n",
    "* [Sci-kit Learn Preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "* [Pandas Interpolate](https://pandas.pydata.org/pandas-docs/version/0.16/generated/pandas.DataFrame.interpolate.html)\n",
    "* [Pandas fillna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk0SxUluJ6R4"
   },
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "\n",
    "# Can be useful under some conditions where you are executing things locally:\n",
    "#from IPython import get_ipython\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOndwAq0J6R7",
    "outputId": "de4b6dc1-af2b-47da-cd62-414fb82f5e47"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive. This step should be done while using Google Colab.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBYx3FfDJ6R9"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhxMn1gfJ6R9",
    "outputId": "e15f4408-1ec9-4b61-b234-8e2eff3259fc"
   },
   "outputs": [],
   "source": [
    "# TODO: Load in the baby data file\n",
    "fname ='/content/drive/MyDrive/MLP_2022/datasets/baby1/subject_k1_w10_hw2.csv'\n",
    "#fname ='/home/fagg/datasets/baby1/subject_k1_w10_hw2.csv'\n",
    "\n",
    "\n",
    "baby_data_raw = # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "uc6N5yQEJ6SB",
    "outputId": "bd8ac4cc-7ed5-486a-c546-b55f45fcd43f"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Call describe() on the data to get summary statistics\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "V8SQgl1oJ6SC",
    "outputId": "d685314c-5d27-4b48-fbc4-0bf1e6b3bd19"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Call head() on the data to observe the first few examples\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7Df0y4dhJ6SE",
    "outputId": "c8c5b600-15b1-4dab-dac7-8f612106671a"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Call tail() on the data to observe the last few examples\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oni_Vf0iJ6SG",
    "outputId": "caf6f21b-7c94-4a85-bdfe-098c937af66e"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Display the column names for the data\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_CKHKVvJ6SI",
    "outputId": "cdf7abb9-ae88-4b56-d51d-072e9fb7166d"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Determine whether any data are NaN. Use isna() and\n",
    "any() to obtain a summary of which features have at \n",
    "least one missing value\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRR_feXVJ6SJ"
   },
   "source": [
    "# Create Pipeline Elements\n",
    "In the lecture, some of the Pipeline components received or returned numpy arrays, while others retceived or returned pandas DataFrames. For this assignment, transform methods for all the Pipeline components will take input as a pandas DataFrame and return a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKV13-NmJ6SK"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Pipeline component object for selecting a subset of specified features\n",
    "\"\"\"\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribs):\n",
    "        self.attribs = attribs\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        PARAMS:\n",
    "            X: is a DataFrame\n",
    "        RETURNS: a DataFrame of the selected attributes\n",
    "        '''\n",
    "        return X[self.attribs]\n",
    " \n",
    "\"\"\" TODO\n",
    "Complete the Pipeline component object for interpolating and filling in \n",
    "gaps within the data. Whenever data are missing inbetween valid values, \n",
    "use interpolation to fill in the gaps. For example,\n",
    "    1.2 NaN NaN 1.5 \n",
    "becomes\n",
    "    1.2 1.3 1.4 1.5 \n",
    "\n",
    "Whenever data are missing on the edges of the data, fill in the gaps\n",
    "with the first available valid value. For example,\n",
    "    NaN NaN 2.3 3.6 3.2 NaN\n",
    "becomes\n",
    "    2.3 2.3 2.3 3.6 3.2 3.2\n",
    "The transform() method you create should fill in the holes and the edge cases.\n",
    "\n",
    "Hint: there are DataFrame methods that will help you implement these features\n",
    "\"\"\"\n",
    "class InterpolationImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method='quadratic'):\n",
    "        self.method = method\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X): # TODO\n",
    "        '''\n",
    "        PARAMS:\n",
    "            X: is a DataFrame\n",
    "        RETURNS: a DataFrame without NaNs\n",
    "        '''\n",
    "        # TODO: Interpolate holes within the data\n",
    "        Xout = \n",
    "        # TODO: Fill in the NaNs on the edges of the data\n",
    "        Xout = \n",
    "        \n",
    "        return Xout\n",
    "    \n",
    "\"\"\" TODO\n",
    "Oftentimes, simple linear interpolation does not produce desirable results.\n",
    "One way to improve our approach is to use a Gaussian kernel, which applies\n",
    "a smoothing function over the data. This smoothing process helps to improve \n",
    "our result from interpolation, reduces the noise the impact of outliers, and \n",
    "generally improves learning.\n",
    "\n",
    "A gaussian kernel is a powerful tool in machine learning - in this case,\n",
    "we're using a Gaussian kernel to build a filter that we convolve over the \n",
    "data. In a kernel, points close to x[t] are used in a weighted\n",
    "average to compute the new (smoothed) value, x'[t]. Here, the three \n",
    "datapoints to the left and right of x[t] are used (along with x[t]) to \n",
    "compute x'[t]. In a Gaussian kernel, we use the Gaussian (normal) \n",
    "distribution to determine what the weight for each point should be in \n",
    "our weighted average. The calculation of this is done for you.\n",
    "\n",
    "Complete the GaussianFilter component object for smoothing specific features\n",
    "using a Gaussian kernel. Here is the example formula for a filter of size k=7:\n",
    "    x'[t] = ( w[0]*x[t-3] + w[1]*x[t-2] + w[2]*x[t-1] + w[3]*x[t]\n",
    "           + w[4]*x[t+1] + w[5]*x[t+2] + w[6]*x[t+3])\n",
    "                \n",
    "This can be implemented similarly to how the derivative is computed, but will\n",
    "require\n",
    "1. padding both ends of x with k/2 instances of the adjacent\n",
    "value, before filtering, to maintain the original timeseries length and \n",
    "smoothness. For example,\n",
    "                1.3 2.1 4.4 4.1 3.2\n",
    "would be padded as\n",
    "    1.3 1.3 1.3 1.3 2.1 4.4 4.1 3.2 3.2 3.2 3.2\n",
    "2. Iterating over the k filter elements (rather than interating over the \n",
    "samples in x)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def computeweights(length=3, sig=1):\n",
    "    ''' PROVIDED\n",
    "    Computes the weights for a Gaussian filter kernel\n",
    "    PARAMS:\n",
    "        length: the number of terms in the filter kernel\n",
    "        sig: the standard deviation (i.e. the scale) of the Gaussian\n",
    "    RETURNS: a list of filter weights for the Gaussian kernel\n",
    "    '''\n",
    "    limit = 2.5\n",
    "    x = np.linspace(-limit, limit, length)\n",
    "    kernel = stats.norm.pdf(x, scale=sig)\n",
    "    \n",
    "    # Return the normalized kernel\n",
    "    return kernel / kernel.sum()\n",
    "\n",
    "class GaussianFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribs=None, kernelsize=3, sig=1):\n",
    "        self.attribs = attribs\n",
    "        # Number of kernel elements \n",
    "        self.kernelsize = kernelsize\n",
    "        \n",
    "        # Check that we have an odd kernel size\n",
    "        if kernelsize % 2 == 0:\n",
    "            raise Exception(\"Expecting an odd kernel size\")\n",
    "\n",
    "        # Standard deviation of the Gaussian\n",
    "        self.sig = sig\n",
    "        # Compute the kernel element values\n",
    "        self.weights = computeweights(length=kernelsize, sig=sig)\n",
    "        print(\"KERNEL WEIGHTS\", self.weights)\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X): # TODO\n",
    "        '''\n",
    "        PARAMS:\n",
    "            X: is a DataFrame\n",
    "        RETURNS: a DataFrame with the smoothed signals\n",
    "        '''\n",
    "        w = self.weights\n",
    "        ks = self.kernelsize\n",
    "        Xout = X.copy()\n",
    "        \n",
    "        # Select all attributes if unspecified\n",
    "        if self.attribs == None:\n",
    "          self.attribs = Xout.columns\n",
    "        \n",
    "        for attrib in self.attribs:\n",
    "            # Extract the numpy vector\n",
    "            vals = Xout[attrib].values\n",
    "            # TODO: pad signal at both the front and end of the vector so that after\n",
    "            #   convolution, the length is the same as the lenght of vals.  Use \n",
    "            #   vals[0] and vals[-1] to pad the front and back, respectively.\n",
    "            #   You may assume that the kernel size is always odd\n",
    "            \n",
    "            nfrontpad = ks // 2 # int division\n",
    "            \n",
    "            # TODO: apply filter\n",
    "            # Implementation is the same as for the DerivativeComputer element, but\n",
    "            #   more general.  You must iterate over the kernel elements.\n",
    "            #   (NOTE: due to the wonky way indexing works in python, you will have\n",
    "            #   specific code for one index & iterate over the remaining k-1 indices)\n",
    "            \n",
    "            \n",
    "            Xout[attrib] = pd.Series(avg)\n",
    "            \n",
    "        return Xout\n",
    "    \n",
    "\"\"\" PROVIDED\n",
    "Pipeline component object for computing the derivative for specified features\n",
    "\"\"\"\n",
    "class DerivativeComputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribs=None, prefix='d_', dt=1.0):\n",
    "        self.attribs = attribs\n",
    "        self.prefix = prefix\n",
    "        self.dt = dt\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        PARAMS:\n",
    "            X: is a DataFrame\n",
    "        RETURNS: a DataFrame with additional features for the derivatives\n",
    "        '''\n",
    "        Xout = X.copy()\n",
    "        if self.attribs == None:\n",
    "            self.attribs = Xout.columns\n",
    "\n",
    "        # Iterate over all of the attributes that we need to compute velocity over\n",
    "        for attrib in self.attribs:\n",
    "            # Extract the numpy array of data\n",
    "            vals = Xout[attrib].values\n",
    "            # Compute the difference between neighboring timeseries elements\n",
    "            diff = vals[1:] - vals[0:-1]\n",
    "            # Take into account the amount of time between timeseries samples\n",
    "            deriv = diff / self.dt\n",
    "            # Add a zero to the end so the resulting velocity vector is the same\n",
    "            #   length as the position vector\n",
    "            deriv = np.append(deriv, 0)\n",
    "            \n",
    "            # Add a new derivative attribute to the DataFrame\n",
    "            attrib_name = self.prefix + attrib\n",
    "            Xout[attrib_name] = pd.Series(deriv)\n",
    "\n",
    "        return Xout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6xl3-AxJ6SS"
   },
   "source": [
    "# Construct Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9DS8hKvJ6ST"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Set up convenience variables. Use the right wrist data as features.\n",
    "\"\"\"\n",
    "#selected_names = TODO\n",
    "selected_names = ['right_wrist_x', 'right_wrist_y', 'right_wrist_z']\n",
    "nselected = len(selected_names)\n",
    "time = baby_data_raw['time'].values\n",
    "# raw data for selected features\n",
    "Xsel_raw = baby_data_raw[selected_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrsxTgpXJ6SU",
    "outputId": "efc6cdfa-8f52-4e56-ea29-380a78d21366"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Create a pipeline that:\n",
    "1. Selects a subset of features specified above\n",
    "2. Fills gaps within the data by linearly interpolating the values \n",
    "   in between existing data and fills the remaining gaps at the edges\n",
    "   of the data with the first or last valid value\n",
    "3. Compute the derivatives of the selected features. The data are \n",
    "   sampled at 50 Hz, therefore, the period or elapsed time (dt) between \n",
    "   the samples is .02 seconds (dt=.02)\n",
    "\"\"\"\n",
    "pipe1 = Pipeline([\n",
    "   \n",
    "])\n",
    "\n",
    "\"\"\" TODO\n",
    "Create a pipeline that:\n",
    "1. Selects a subset of features specified above\n",
    "2. Fills gaps within the data by linearly interpolating the values \n",
    "   in between existing data and fills the remaining gaps at the edges\n",
    "   of the data with the first or last valid value\n",
    "3. Smooths the data with a Gaussian Filter. Use a standard deviation \n",
    "   of 2 and a kernel size of 7 for the filter\n",
    "4. Compute the derivatives of the selected features. The data are \n",
    "   sampled at 50 Hz, therefore, the period or elapsed time (dt) between \n",
    "   the samples is .02 seconds (dt=.02)\n",
    "\"\"\"\n",
    "pipe2 = Pipeline([\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "h4CXW9DIJ6SV",
    "outputId": "b805289f-16d7-4423-8039-60b04821cb44"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Fit both Pipelines to the data and transform the data\n",
    "\"\"\"\n",
    "baby_data1 = # TODO\n",
    "baby_data2 = # TODO\n",
    "\n",
    "\"\"\" TODO\n",
    "Display the summary statistics for the pre-processed data\n",
    "from both pipelines\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "RpUWajl_J6SY",
    "outputId": "2ad46997-1bcb-497a-fd1a-1e76ff2e3d67"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "IpB3CdkrJ6Sa",
    "outputId": "beb8b42b-0209-43de-c560-37b6a4d08526"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Display the first 10 values for the pre-processed data\n",
    "from both pipelines\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "ClCHreQWJ6Sb",
    "outputId": "f419e787-e06c-4773-8c11-cb2bd499ac36"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "7X6BbtubJ6Sc",
    "outputId": "683677aa-cb08-45d6-8ecd-19827e369831"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Display the last 10 values for the pre-processed data\n",
    "from both pipelines\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "WsPvpbY7J6Sd",
    "outputId": "769d0327-d7e3-4d9f-ad3b-ba00fb9fff3d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ee4NXVQWJ6Sf",
    "outputId": "52572bbe-4eda-474a-ae83-65ba7f0eae41"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Construct plots comparing the raw data to the pre-processed data \n",
    "for each selected feature from both pipelines. For each selected \n",
    "feature, create a figure displaying the raw data and the cleaned \n",
    "data in the same subplot. The raw data should be shifted upwards \n",
    "to clearly observe where the gaps are filled in the cleaned data. \n",
    "There should be three subplots per feature figure. Each subplot \n",
    "is in a separate row.\n",
    "    subplot(1) will compare the original raw data to the pipeline1 \n",
    "               pre-processed data\n",
    "    subplot(2) will compare the original raw data to the pipeline2 \n",
    "               pre-processed data\n",
    "    subplot(3) will compare pipeline1 to pipeline2. Set the x limit \n",
    "               to 45 and 55 seconds\n",
    "For all subplots, include axis labels, legends and titles.\n",
    "\"\"\"\n",
    "xlim = [45, 55]\n",
    "xsel_clean1 = baby_data1.values\n",
    "xsel_clean2 = baby_data2.values\n",
    "\n",
    "for f, fname in enumerate(selected_names):\n",
    "    fig, axs = plt.subplots(len(selected_names),1)\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    # PIPELINE 1\n",
    "    # TODO\n",
    "\n",
    "    # PIPELINE 2\n",
    "    # TODO\n",
    "\n",
    "    # PIPELINE 1 VS PIPELINE 2\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eK9TVD3yJ6Sg",
    "outputId": "464e34d7-8990-4203-8456-702968b3fe0b"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Construct plots for each feature presenting the feature and its \n",
    "derivative from both pipelines. Each figure should have \n",
    "3 subplots:\n",
    "    1: the pipeline1 feature data and cooresponding derivative \n",
    "    2: the pipeline2 feature data and corresponding derivative\n",
    "    3: pipeline1 derivative and pipeline2 derivative. Set the x limit \n",
    "       to 8 and 12 seconds.\n",
    "For all subplots, include axis labels, legends and titles.\n",
    "\"\"\"\n",
    "xlim = [8, 12]\n",
    "\n",
    "for f, fname in enumerate(selected_names):\n",
    "    d_fname = 'd_' + fname\n",
    "    fig, axs = plt.subplots(3,1,)\n",
    "    #fig.subplots_adjust(hspace=.35)\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    # PIPELINE 1\n",
    "    # TODO\n",
    "\n",
    "    # PIPELINE 2\n",
    "    # TODO\n",
    "    \n",
    "    # DERIVATIVES\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkrrkKAlJ6Si"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "152px",
    "width": "208px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
